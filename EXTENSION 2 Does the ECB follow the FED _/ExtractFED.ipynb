{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**FED extract**"
      ],
      "metadata": {
        "id": "1F1eccFQIwwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea here is just to scrap the communications from the FED between 2012 and to apply BERT and compare monetary policies to see whether the FED's decisions influence the ECB."
      ],
      "metadata": {
        "id": "iyc9gd_0IUqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"https://www.federalreserve.gov\"\n",
        "\n",
        "data_log_response = requests.get(BASE_URL + \"/monetarypolicy/materials/assets/final-recent.json\")\n",
        "if data_log_response.ok:\n",
        "    data_log = [x for x in data_log_response.json()[\"mtgitems\"] if x[\"type\"] == \"Mn\"]\n",
        "\n",
        "    filtered_data = []\n",
        "\n",
        "    for item in data_log:\n",
        "        html_item = {\n",
        "            \"Date\": item[\"d\"],\n",
        "            \"Type\": \"Minute\",\n",
        "            \"Release Date\": item.get(\"dt\", \"Not Available\"),\n",
        "            \"URL\": next(\n",
        "                (file[\"url\"] for file in item[\"files\"] if file[\"name\"] == \"HTML\"), None\n",
        "            ),\n",
        "        }\n",
        "        filtered_data.append(html_item)\n",
        "\n",
        "    # Scrape and process the text of the FED communications\n",
        "    for item in filtered_data:\n",
        "        if item[\"URL\"] is not None:\n",
        "            full_url = BASE_URL + item[\"URL\"]\n",
        "            response = requests.get(full_url)\n",
        "            if response.ok:\n",
        "                doc = BeautifulSoup(response.text, features=\"html5lib\")\n",
        "                article_div = doc.find(\"div\", id=\"article\")\n",
        "                if article_div:\n",
        "                    full_text = article_div.text.strip()\n",
        "                    # Extract the section of the text, for that we searched manually the pattern of each text\n",
        "                    start_index = full_text.find(\"Developments in Financial Markets and Open Market Operations\")\n",
        "                    end_index = full_text.find(\"Notation Vote\")\n",
        "\n",
        "                    if start_index == -1:  # If the first section is not found, look for the second one (second patterns)\n",
        "                        start_index = full_text.find(\"Developments in Financial Markets and the Federal Reserve's Balance Sheet\")\n",
        "\n",
        "                    if start_index != -1 and end_index != -1:\n",
        "                        relevant_text = full_text[start_index:end_index]\n",
        "                    else:\n",
        "                        relevant_text = \"\"\n",
        "                    item[\"Text\"] = relevant_text\n",
        "\n",
        "    # Turn into dataframe to applicate Bert on it\n",
        "    communications_df = pd.json_normalize(filtered_data).drop(columns=[\"URL\"])[\n",
        "        [\"Date\", \"Release Date\", \"Type\", \"Text\"]\n",
        "    ]\n",
        "\n",
        "    communications_df.to_csv(\"communications.csv\", index=False)"
      ],
      "metadata": {
        "id": "bvWDIQFqLrwW"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}